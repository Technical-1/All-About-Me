---
// WebLLMPreloader.astro
// Preloads WebLLM model weights in the background on any page
// This ensures the model is cached in IndexedDB before users visit the chat page
---

<script>
  // Only run preloading once per session
  const PRELOAD_KEY = 'webllm_preload_started';
  const PRELOAD_FAILED_KEY = 'webllm_preload_failed';

  async function checkFullWebGPUSupport(): Promise<boolean> {
    // Skip on mobile devices - WebGPU support is unreliable
    const isMobile = /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent);
    if (isMobile) {
      console.log('[WebLLM Preloader] Mobile device detected, skipping preload');
      return false;
    }

    // Check basic WebGPU API
    if (typeof navigator === 'undefined' || !navigator.gpu) {
      return false;
    }

    try {
      const adapter = await navigator.gpu.requestAdapter();
      if (!adapter) {
        return false;
      }

      // Actually request a device to ensure full support
      const device = await adapter.requestDevice();
      if (!device) {
        return false;
      }

      return true;
    } catch (e) {
      return false;
    }
  }

  async function preloadWebLLM() {
    // Skip if already started this session
    if (sessionStorage.getItem(PRELOAD_KEY)) {
      return;
    }

    // Skip if previously failed this session (don't retry failures)
    if (sessionStorage.getItem(PRELOAD_FAILED_KEY)) {
      return;
    }

    // Thorough WebGPU check including mobile detection
    const supported = await checkFullWebGPUSupport();
    if (!supported) {
      console.log('[WebLLM Preloader] WebGPU not fully supported, skipping preload');
      return;
    }

    // Mark as started to prevent duplicate preloads
    sessionStorage.setItem(PRELOAD_KEY, 'true');
    console.log('[WebLLM Preloader] Starting background model preload...');

    try {
      // Dynamically import WebLLM
      const { CreateMLCEngine } = await import('@mlc-ai/web-llm');

      // Create engine with progress callback - this downloads and caches the model
      const engine = await CreateMLCEngine(
        'SmolLM2-1.7B-Instruct-q4f16_1-MLC',
        {
          initProgressCallback: (report) => {
            // Log progress but don't show UI (background operation)
            if (report.progress && report.progress % 20 === 0) {
              console.log(`[WebLLM Preloader] ${report.text}`);
            }
          }
        }
      );

      // Store the engine reference globally so ChatInterface can use it
      (window as any).__webllmEngine = engine;
      console.log('[WebLLM Preloader] Model preloaded and cached successfully!');
    } catch (err) {
      console.warn('[WebLLM Preloader] Background preload failed:', err);
      // Mark as failed so we don't retry this session
      sessionStorage.setItem(PRELOAD_FAILED_KEY, 'true');
      sessionStorage.removeItem(PRELOAD_KEY);
    }
  }

  // Start preloading after page becomes interactive
  if (document.readyState === 'complete') {
    // Use requestIdleCallback if available for lower priority
    if ('requestIdleCallback' in window) {
      (window as any).requestIdleCallback(() => preloadWebLLM(), { timeout: 5000 });
    } else {
      setTimeout(preloadWebLLM, 1000);
    }
  } else {
    window.addEventListener('load', () => {
      if ('requestIdleCallback' in window) {
        (window as any).requestIdleCallback(() => preloadWebLLM(), { timeout: 5000 });
      } else {
        setTimeout(preloadWebLLM, 1000);
      }
    });
  }
</script>
